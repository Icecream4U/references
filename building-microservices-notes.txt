Strong cohesion and loose coupling

Any technology that pushes us to expose internal representation detail should be avoided. Share behaviour not representation.

Orchestration Versus Choreography

keep your middleware dumb, and keep the smarts in the endpoints

don’t violate DRY within a microservice, but be relaxed about violating DRY across all services. The evils of too much coupling between services are far worse than the problems caused by code duplication

----

Eventual consistency (local retry until it works) VS Distributed transaction (transaction manager -> 2 phase commit)

class-responsibility-collaboration (CRC) cards with services used as class

----

Avoid "configuration drift": the code in source control no longer reflects the configuration of the running host
We can resolve it with "Immutable servers"

----

A test suite with flaky tests can become a victim of what Diane Vaughan calls the normalization of deviance. The idea that over time we can become so accustomed to things being wrong that we start to accept them as being normal and not a problem 
(Diane Vaughan, The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA (Chicago: University of Chicago Press, 1996))

The best balance I have found is to treat the end-to-end test suite as a shared codebase, but with joint ownership. Teams are free to check in to this suite, but the ownership of the health of the suite has to be shared between the teams developing the services themselves. If you want to make extensive use of end-to-end tests with multiple teams I think this approach is essential, and yet I have seen it done very rarely, and never without issue

Test Journeys, Not Stories -> Consumer driven Contracts

Using blue/green deployments to separate deployment from release: Deploy new versione -> run smoke test against it -> Redirect traffic

Canary releasing: we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected

Cross-functional requirements: Nonfunctional requirements is an umbrella term used to describe those characteristics your system exhibits that cannot simply be implemented like a normal feature. They include aspects like the acceptable latency of a web page, the number of users a system should support, how accessible your user interface should be to people with disabilities, or how secure your customer data should be.

----

there is an old adage that 80% of software features are never used. Now I can’t comment on how accurate that figure is, but as someone who has been developing software for nearly 20 years, I know that I have spent a lot of time on features that never actually get used. Wouldn’t it be nice to know what they are?
we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push out a new version of the website, and find that the number of searches by genre has gone up significantly on the catalog service. Is that a problem, or expected?

synthetic transaction to enable semantic monitoring to better understand metrics before going on production

correlationsID (or Zipkin) to trace the results of an entry call in your system and all subsequent call the many services => mandatory in event-drive architecture patterns

be aware of the cascade: a service that go down and throws down every service that call it

With services collaborating in lots of different ways to provide capabilities to users using multiple interfaces, you need to view the system in a holistic way.
You should try to write your logs out in a standard format. You definitely want to have all your metrics in one place, and you may want to have a list of standard names for your metrics too; it would be very annoying for one service to have a metric called ResponseTime, and another to have one called RspTimeSecs, when they mean the same thing.

So why handle operational and business metrics in the same way? Ultimately, both types of things break down to events that say something happened at X. So, if we can unify the systems we use to gather, aggregate, and store these events, and make them available for reporting, we end up with a much simpler architecture.

----

Netflix designed the organizational structure for the system architecture it wanted.
Amazon's two-pizza teams, where no team should be so big that it could not be fed with two pizzas

----

I’m amazed at how many organizations put processes and controls in place to try to stop failure from occurring, but put little to no thought into actually making it easier to recover from failure in the first place.
I saw one example of this thinking while spending some time on the Google campus many years ago. In the reception area of one of the buildings in Mountain View was an old rack of machines, there as a sort of exhibit. I noticed a couple of things. First, these servers weren’t in server enclosures, they were just bare motherboards slotted into the rack. The main thing I noticed, though, was that the hard drives were attached by velcro. I asked one of the Googlers why that was. “Oh,” he said, “the hard drives fail so much we don’t want them screwed in. We just rip them out, throw them in the bin, and velcro in a new one.
Why not use a bare motherboard with cheaper components (and some velcro) like Google did, rather than worrying too much about the resiliency of a single node?

When it comes to considering if and how to scale out your system to better handle load or failure, start by trying to understand the following requirements:
- Response time/latency (ex: We expect the website to have a 90th-percentile response time of 2 seconds when handling 200 concurrent connections per second)
- Availability: Can you expect a service to be down? Is this considered a 24/7 service? Some people like to look at periods of acceptable downtime when measuring availabil‐ ity, but how useful is this to someone calling your service? I should either be able to rely on your service responding or not. Measuring periods of downtime is really more useful from a historical reporting angle.
- Durability of data: How much data loss is acceptable? How long should data be kept for? (ex: you might choose to keep user session logs for a year or less to save space, but your financial transaction records might need to be kept for many years)

Understand the impact of each outage and work out how to properly degrade functionality. If the shopping cart service is unavailable, we’re probably in a lot of trouble, but we could still show the web page with the listing. Perhaps we just hide the shopping cart or replace it with an icon saying “Be Back Soon!”. 
With a single, monolithic application, we don’t have many decisions to make. System health is binary. But with a microservice architecture, we need to consider a much more nuanced situation. The right thing to do in any situation is often not a technical decision. We might know what is technically possible when the shopping cart is down, but unless we understand the business context we won’t understand what action we should be taking.

We discovered the hard way that systems that just act slow are much harder to deal with than systems that just fail fast. In a distributed system, latency kills.

--
The Antifragile Organization:
Some organizations would be happy with game days, where failure is simulated by systems being switched off and having the various teams react. During my time at Google, this was a fairly common occurrence for various systems, and I certainly think that many organizations could benefit from having these sorts of exercises reg‐ ularly. Google goes beyond simple tests to mimic server failure, and as part of its annual DiRT (Disaster Recovery Test) exercises it has simulated large-scale disasters such as earthquakes. Netflix also takes a more aggressive approach, by writing pro‐ grams that cause failure and running them in production on a daily basis (Netflix's Simian Army). Embracing and inciting failure through software, and building systems that can handle it, is only part of what Netflix does. It also understands the importance of learning from the failure when it occurs, and adopting a blameless culture when mistakes do happen. Developers are further empowered to be part of this learning and evolving process, as each developer is also responsible for managing his or her production services.

So what do we need to do to handle failure in our systems?

- Timeouts: Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream sys‐ tem to actually be down?
Wait too long to decide that a call has failed, and you can slow the whole system down. Time out too quickly, and you’ll consider a call that might have worked as failed. Have no timeouts at all, and a downstream system being down could hang your whole system.
Put timeouts on all out-of-process calls, and pick a default timeout for everything. Log when timeouts occur, look at what happens, and change them accordingly.

- Circuit Breakers: How you implement a circuit breaker depends on what a failed request means, but when I’ve implemented them for HTTP connections I’ve taken failure to mean either a timeout or a 5XX HTTP return code. In this way, when a downstream resource is down, or timing out, or returning errors, after a certain threshold is reached we auto‐ matically stop sending traffic and start failing fast. And we can automatically start again when things are healthy.

- Bulkheads: a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be sealed off to protect the rest of the ship. So if the ship springs a leak, you can close the bulkhead doors. You lose part of the ship, but the rest of it remains intact. 
Separation of concerns can also be a way to implement bulkheads. By teasing apart functionality into separate microservices, we reduce the chance of an outage in one area affecting another. Look at all the aspects of your system that can go wrong, both inside your microservi‐ ces and between them. Do you have bulkheads in place? I’d suggest starting with sep‐ arate connection pools for each downstream connection at the very least. You may want to go further, however, and consider using circuit breakers too.
We can think of our circuit breakers as an automatic mechanism to seal a bulkhead, to not only protect the consumer from the downstream problem, but also to poten‐ tially protect the downstream service from more calls that may be having an adverse impact. Given the perils of cascading failure, I’d recommend mandating circuit break‐ ers for all your synchronous downstream calls.

In many ways, bulkheads are the most important of these three patterns. Timeouts and circuit breakers help you free up resources when they are becoming constrained, but bulkheads can ensure they don’t become constrained in the first place. Hystrix allows you, for example, to implement bulkheads that actually reject requests in certain conditions to ensure that resources don’t become even more saturated; this is known as load shedding. Sometimes rejecting a request is the best way to stop an important system from becoming overwhelmed and being a bottleneck for multiple upstream services.

Isolation: The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream server to be offline, upstream services are less likely to be affec‐ ted by outages, planned or unplanned.
There is another benefit to increasing isolation between services. When services are isolated from each other, much less coordination is needed between service owners. The less coordination needed between teams, the more autonomy those teams have, as they are able to operate and evolve their services more freely.
--

Idempotency: In idempotent operations, the outcome doesn’t change after the first application, even if the operation is subsequently applied multiple times. If operations are idempotent, we can repeat the call multiple times without adverse impact. This is very useful when we want to replay messages that we aren’t sure have been processed, a common way of recovering from error. Some of the HTTP verbs, such as GET and PUT, are defined in the HTTP specifica‐ tion to be idempotent, but for that to be the case, they rely on your service handling these calls in an idempotent manner. If you start making these verbs nonidempotent, but callers think they can safely execute them repeatedly, you may get yourself into a mess

As Jeff Dean said in his presentation “Challenges in Building Large-Scale Information Retrieval Systems” (WSDM 2009 conference), you should “design for ~10× growth, but plan to rewrite before ~100×.” At certain points, you need to do something pretty radical to support the next level of growth.
The need to change our systems to deal with scale isn’t a sign of failure. It is a sign of success.

--
Caching
In client-side caching, the client stores the cached result. The client gets to decide when (and if) it goes and retrieves a fresh copy. Ideally, the downstream service will provide hints to help the client understand what to do with the response, so it knows when and if to make a new request. 
With proxy caching, a proxy is placed between the client and the server. A great example of this is using a reverse proxy or content delivery network (CDN). 
With server-side caching, the server handles caching responsibility, perhaps making use of a system like Redis or Memcache, or even a simple in-memory cache.

HTTP caching: ETags, Expires, and cache-control

Caching for write: write data in a persisted cache and periodically write those data in the storage. Could be useful for burst write of duplicated data or to decouple the write process. Wheter storage goes down the cache will work like a queue to prevent data losing and let system work.

Caching for resilience: Caching can be used to implement resiliency in case of failure. With client-side cach‐ ing, if the downstream service is unavailable, the client could decide to simply use cached but potentially stale data. We could also use something like a reverse proxy to serve up stale data. For some systems, being available even with stale data is better than not returning a result at all, but that is a judgment call you’ll have to make. Obviously, if we don’t have the requested data in the cache, then we can’t do much to help, but there are ways to mitigate this.
A technique I saw used at the Guardian, and subsequently elsewhere, was to crawl the existing live site periodically to generate a static version of the website that could be served in the event of an outage. Although this crawled version wasn’t as fresh as the cached content served from the live system, in a pinch it could ensure that a version of the site would get displayed.

Hiding the origin: With a normal cache, if a request results in a cache miss, the request goes on to the origin to fetch the fresh data with the caller blocking, waiting on the result. In the normal course of things, this is to be expected. But if we suffer a massive cache miss, perhaps because an entire machine (or group of machines) that provide our cache fail, a large number of requests will hit the origin.
For those services that serve up highly cachable data, it is common for the origin itself to be scaled to handle only a fraction of the total traffic, as most requests get served out of memory by the caches that sit in front of the origin. If we suddenly get a thun‐ dering herd due to an entire cache region vanishing, our origin could be pummelled out of existence.
One way to protect the origin in such a situation is never to allow requests to go to the origin in the first place. Instead, the origin itself populates the cache asynchro‐ nously when needed, as shown in Figure 11-7. If a cache miss is caused, this triggers an event that the origin can pick up on, alerting it that it needs to repopulate the cache. So if an entire shard has vanished, we can rebuild the cache in the background. We could decide to block the original request waiting for the region to be repopula‐ ted, but this could cause contention on the cache itself, leading to further problems. It’s more likely if we are prioritizing keeping the system stable that we would fail the original request, but it would fail fast.
This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up when parts of it fail. By failing requests fast, and ensur‐ ing we don’t take up resources or increase latency, we avoid a failure in our cache from cascading downstream and give ourselves a chance to recover.

Keep it simple! Be careful about caching in too many places! The more caches between you and the source of fresh data, the more stale the data can be, and the harder it can be to deter‐ mine the freshness of the data that a client eventually sees. This can be especially problematic with a microservice architecture where you have multiple services involved in a call chain. Again, the more caching you have, the harder it will be to assess the freshness of any piece of data. So if you think a cache is a good idea, keep it simple, stick to one, and think carefully before adding more!

Cache poisoning: wheter something goes wrong (like you release an Expires:never header!) there are also browser's cache and ISP's cache to consider. Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong (to avoid changing url to permit users to read fresh data).
--

Austoscaling: reactive autoscaling vs predictive autoscaling. Maybe better to use a mix of both!

--

CAP Theorem: AP vs CP. As the people building the system, we know the trade-off exists. We know that AP systems scale more easily and are simpler to build, and we know that a CP system will require more work due to the challenges in supporting distributed consistency. But we may not understand the business impact of this trade-off. For our inventory system, if a record is out of date by five minutes, is that OK? If the answer is yes, an AP system might be the answer. But what about the balance held for a customer in a bank? Can that be out of date? Without knowing the context in which the operation is being used, we can’t know the right thing to do. Knowing about the CAP theorem just helps you understand that this trade-off exists and what questions to ask.
Our system as a whole doesn’t need to be either AP or CP. Our catalog could be AP, as we don’t mind too much about a stale record. But we might decide that our inventory service needs to be CP, as we don’t want to sell a customer something we don’t have and then have to apologize later.
But individual services don’t even need to be CP or AP.
Let’s think about our points balance service, where we store records of how many loyalty points our customers have built up. We could decide that we don’t care if the balance we show for a customer is stale, but that when it comes to updating a balance we need it to be consistent to ensure that customers don’t use more points than they have available. Is this microservice CP, or AP, or is it both? Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities.
Let’s revisit our inventory system. This maps to real-world, physical items. We keep a count in our system of how many albums we have. At the start of the day we had 100 copies of Give Blood by The Brakes. We sold one. Now we have 99 copies. Easy, right? By what happens if when the order was being sent out, someone knocks a copy of the album onto the floor and it gets stepped on and broken? What happens now? Our systems say 99, but there are 98 copies on the shelf.
What if we made our inventory system AP instead, and occasionally had to contact a user later on and tell him that one of his items is actually out of stock? Would that be the worst thing in the world? It would certainly be much easier to build, scale, and ensure it is correct.
We have to recognize that no matter how consistent our systems might be in and of themselves, they cannot know everything that happens, especially when we’re keeping records of the real world. This is one of the main reasons why AP systems end up being the right call in many situations. Aside from the complexity of building CP systems, they can’t fix all our problems anyway.

--
Service Discovery

DNS: I have seen a convention-based domain template work well. For example, we might have a template defined as <servicename>-<environment>.musiccorp.com, giving us entries like accounts-uat.musiccorp.com or accounts-dev.musiccorp.com. 
For those situations where you need more than one instance of a host, have DNS entries resolve to load balancers that can handle putting individual hosts into and out of service as appropriate

Dynamic Service Registry: Zookeper vs Consul vs Eureka vs build-your-own-on-AWS

-----------
## Links ##
-----------
DDD Bounded Context Explained: https://web.archive.org/web/20130101104734/http://www.sapiensworks.com/blog/post/2012/04/17/DDD-The-Bounded-Context-Explained.aspx

Richardson REST maturity model: https://martinfowler.com/articles/richardsonMaturityModel.html

Tolerant Reader: https://martinfowler.com/bliki/TolerantReader.html

Semantic versioning: https://semver.org/

Expand and contract pattern: https://martinfowler.com/bliki/ParallelChange.html

Strangler application pattern: https://martinfowler.com/bliki/StranglerFigApplication.html

Test doubles: https://www.martinfowler.com/bliki/TestDouble.html

Eradicating non determinism in tests: https://martinfowler.com/articles/nonDeterminism.html

Enterprise integration using REST: https://martinfowler.com/articles/enterpriseREST.html

Salted password hashing: https://crackstation.net/hashing-security.htm#properhashing

Fallacies of distributed computing: https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing

The antifragile organization: https://queue.acm.org/detail.cfm?id=2499552

-----------
## Books ##
-----------
REST in Practice (O’Reilly)

Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions (Addison Wesley)

Working Effectively with Legacy Code

Refactoring Databases by Scott J. Ambler and Pramod J. Sadalage (Addison-Wesley)

Continuous Delivery by Jez Humble and Dave Farley 

Agile Testing by Lisa Crispin and Janet Gregory

Succeeding with Agile by Mike Cohn

Growing Object-Oriented Software, Guided by Tests, by Steve Freeman and Nat Pryce

Information Dashboard Design: Displaying Data for At-a-Glance Monitoring by Steven Few

Cryptography Engineering by Niels Ferguson, Bruce Schneier, and Tadayoshi Kohno

Exploring the Duality Between Product and Organizational Architectures

Antifragile by Nassim Taleb

Release it! by Michael Nygard

REST In Practice

---------
## Cit ##
---------
Postel’s Law (otherwise known as the robustness principle): “Be conservative in what you do, be liberal in what you accept from others.” [https://tools.ietf.org/html/rfc761]

Melvin Conway’s paper How Do Committees Invent, published in Datamation maga‐ zine in April 1968, observed that:
Any organization that designs a system (defined more broadly here than just informa‐ tion systems) will inevitably produce a design whose structure is a copy of the organi‐ zation’s communication structure.
This statement is often quoted, in various forms, as Conway’s law. Eric S. Raymond summarized this phenomenon in The New Hacker’s Dictionary (MIT Press) by stating “If you have four groups working on a compiler, you’ll get a 4-pass compiler.”

-------------
## Advices ##
-------------
“Friends don’t let friends write their own crypto”
“Friends don’t let friends write their own distributed consistent data store.”
"Friends don't let friends write their own distributed coordination systems."

----------------------
## Personal Advices ##
----------------------
Keep an eye on the reality: you should build the mathematically perfect system which will be used by users which will try to do the worst things ever with your system or you are mapping real world staff that could change out of your control. The cost and the effort to be mathematically perfect worth the risk of beying fucked up by users or reality? Maybe it's better to KISS and do certain operation by hand instead of have a complexity moltiplicator. 
Remember JP Rainsberger: avoid accidental complexity is the key!

-----------
## Tools ##
-----------
Structure 101
Schema spy: http://schemaspy.sourceforge.net/
https://www.packer.io/

Mountebank for test doubles: http://www.mbtest.org/

Pact (consumer-driven testing tool): https://docs.pact.io/

Zed attack proxy (ZAD): https://owasp.org/www-project-zap/

Netflix’s Hystrix library (JVM circuit breaker): https://github.com/Netflix/Hystrix

Consul

https://github.com/Netflix/eureka

